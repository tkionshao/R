"accept-encoding"="gzip, deflate, br",
"cache-control"="max-age=0"
)
ua <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.366"
curl_handle <- getCurlHandle()
install.packages("devtools")
install.packages("tidyRSS")
install.packages("XML")
install.packages("tidyRSS")
curl_handle <- getCurlHandle()
install.packages("devtools")
install.packages("tidyRSS")
install.packages("XML")
install.packages("RCurl")
install.packages("plyr")
install.packages("wordcloud")
install.packages("wordcloud2")
library(tidyRSS)
library(XML)
library(RCurl)
library( jiebaR)
library(stringr)
library(plyr)
library(wordcloud)
library(wordcloud2)
curl_handle <- getCurlHandle()
curlSetOpt(cookiejar="cookies.txt",
useragent = ua,
followlocation = TRUE,
curl=curl_handle
)
ua <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.366"
myHttpHeader <- c(
"User-Agent"=ua,
"accept"="text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
"accept-Language"="en-US,en;q=0.9,ja;q=0.8,zh-TW;q=0.7,zh;q=0.6",
"accept-encoding"="gzip, deflate, br",
"cache-control"="max-age=0"
)
curl_handle <- getCurlHandle()
curlSetOpt(cookiejar="cookies.txt",
useragent = ua,
followlocation = TRUE,
curl=curl_handle
)
data
data <- list()
i <- 1
for( link in rss$item_link ){
html_doc <- htmlParse(getURL(link, curl = curl_handle), encoding = "UTF-8")
article_item <- xpathSApply(html_doc, "//div[@class='ndArticle_margin']/p", xmlValue)
article_item <- gsub("\\s+", "", article_item)
article_item <- gsub(" $", "", article_item)
data[i] <- article_item
i <- i+1
t <- sample(2:5,1)
Sys.sleep(t)
}
data
data <- list()
i <- 1
for( link in rss$item_link ){
html_doc <- htmlParse(getURL(link, curl = curl_handle), encoding = "UTF-8")
article_item <- xpathSApply(html_doc, "//*[@id='article']/div[2]/div/main/article/div/div[2]/article/div/p", xmlValue)
article_item <- gsub("\\s+", "", article_item)
article_item <- gsub(" $", "", article_item)
data[i] <- article_item
i <- i+1
t <- sample(2:5,1)
Sys.sleep(t)
}
data
rss$feed_title # RSS標題
rss$item_title # 文章標題
View(rss$item_title)
View(date)
View(data)
data <- list()
i <- 1
for( link in rss$item_link ){
html_doc <- htmlParse(getURL(link, curl = curl_handle), encoding = "UTF-8")
article_item <- xpathSApply(html_doc, "//*[@id='article']/div[2]/div/main/article/div/div[2]/article/div/p", xmlValue)
article_item <- gsub("\\s+", "", article_item)
article_item <- gsub(" $", "", article_item)
data[i] <- article_item
i <- i+1
t <- sample(2:5,1)
Sys.sleep(t)
}
View(data)
library(tidyRSS)
rss_url <- 'https://tw.appledaily.com/rss/newcreate/kind/rnews/type/104'
rss <- tidyRSS::tidyfeed(feed = rss_url)
rss$feed_title # RSS標題
rss$feed_link # RSS超連結
rss$feed_description # 說明
rss$feed_language # 語系
rss$item_title # 文章標題
rss$item_link # 文章超連結
View(rss$item_title)
Sys.setlocale(category='LC_ALL', locale='C')
myHttpHeader <- c(
"User-Agent"=ua,
"accept"="text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
"accept-Language"="en-US,en;q=0.9,ja;q=0.8,zh-TW;q=0.7,zh;q=0.6",
"accept-encoding"="gzip, deflate, br",
"cache-control"="max-age=0"
)
curl_handle <- getCurlHandle()
curlSetOpt(cookiejar="cookies.txt",
useragent = ua,
followlocation = TRUE,
curl=curl_handle
)
"Accept-Charset"="UTF8,utf-8;q=0.7,*;q=0.7"
"Connection"="keep-alive"
data <- list()
i <- 1
for( link in rss$item_link ){
html_doc <- htmlParse(getURL(link, curl = curl_handle), encoding = "UTF-8")
article_item <- xpathSApply(html_doc, "//*[@id='article']/div[2]/div/main/article/div/div[2]/article/div/p", xmlValue)
article_item <- gsub("\\s+", "", article_item)
article_item <- gsub(" $", "", article_item)
data[i] <- article_item
i <- i+1
t <- sample(2:5,1)
Sys.sleep(t)
}
data
library(tidyRSS)
library(XML)
library(RCurl)
library( jiebaR)
library(stringr)
library(plyr)
library(wordcloud)
library(wordcloud2)
# start to parser
library(tidyRSS)
rss_url <- 'https://tw.appledaily.com/rss/newcreate/kind/rnews/type/104'
rss <- tidyRSS::tidyfeed(feed = rss_url)
rss$feed_title # RSS標題
rss$feed_link # RSS超連結
rss$feed_description # 說明
rss$feed_language # 語系
rss$item_title # 文章標題
rss$item_link # 文章超連結
View(rss$item_title)
#    XPath: //*[@id="article"]/div[2]/div/main/article/div/div[2]/article/div/p
#    //*[@id="article"]/div[2]/div/main/article/div/div[2]/article/div
#    //*[@id="article"]/div[2]/div/main/article/div/div[2]/article
#    //*[@id="article"]/div[2]/div/main/article/div/div[2]/article/div
#    //*[@id="article"]/div[2]/div/main/article/div/div[2]/article/div/p
Sys.getlocale()
Sys.setlocale(category='LC_ALL', locale='C')
# Modify by myself
ua <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.366"
myHttpHeader <- c(
"User-Agent"=ua,
"accept"="text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
"accept-Language"="en-US,en;q=0.9,ja;q=0.8,zh-TW;q=0.7,zh;q=0.6",
"accept-encoding"="gzip, deflate, br",
"cache-control"="max-age=0"
)
curl_handle <- getCurlHandle()
curlSetOpt(cookiejar="cookies.txt",
useragent = ua,
followlocation = TRUE,
curl=curl_handle
)
"Accept-Charset"="UTF8,utf-8;q=0.7,*;q=0.7"
"Connection"="keep-alive"
data <- list()
i <- 1
for( link in rss$item_link ){
html_doc <- htmlParse(getURL(link, curl = curl_handle), encoding = "UTF-8")
article_item <- xpathSApply(html_doc, "//*[@id='article']/div[2]/div/main/article/div/div[2]/article/div/p", xmlValue)
article_item <- gsub("\\s+", "", article_item)
article_item <- gsub(" $", "", article_item)
data[i] <- article_item
i <- i+1
t <- sample(2:5,1)
Sys.sleep(t)
}
library(tidyRSS)
rss_url <- 'https://tw.appledaily.com/rss/newcreate/kind/rnews/type/104'
rss <- tidyRSS::tidyfeed(feed = rss_url)
rss$feed_title # RSS標題
rss$feed_link # RSS超連結
rss$feed_description # 說明
rss$feed_language # 語系
rss$item_title # 文章標題
rss$item_link # 文章超連結
View(rss$item_title)
Sys.setlocale(category='LC_ALL', locale="")
rss_url <- 'https://tw.appledaily.com/rss/newcreate/kind/rnews/type/104'
rss <- tidyRSS::tidyfeed(feed = rss_url)
rss$feed_title # RSS標題
rss$feed_link # RSS超連結
rss$feed_description # 說明
rss$feed_language # 語系
rss$item_title # 文章標題
rss$item_link # 文章超連結
View(rss$item_title)
ua <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.366"
myHttpHeader <- c(
"User-Agent"=ua,
"accept"="text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
"accept-Language"="en-US,en;q=0.9,ja;q=0.8,zh-TW;q=0.7,zh;q=0.6",
"accept-encoding"="gzip, deflate, br",
"cache-control"="max-age=0",
"Connection"="keep-alive",
"Accept-Charset"="UTF8,utf-8;q=0.7,*;q=0.7"
)
curl_handle <- getCurlHandle()
curlSetOpt(cookiejar="cookies.txt",
useragent = ua,
followlocation = TRUE,
curl=curl_handle
)
Sys.getlocale()
Sys.setlocale(category='LC_ALL', locale='C')
data <- list()
i <- 1
for( link in rss$item_link ){
html_doc <- htmlParse(getURL(link, curl = curl_handle), encoding = "UTF-8")
article_item <- xpathSApply(html_doc, "//*[@id='article']/div[2]/div/main/article/div/div[2]/article/div/p", xmlValue)
article_item <- gsub("\\s+", "", article_item)
article_item <- gsub(" $", "", article_item)
data[i] <- article_item
i <- i+1
t <- sample(2:5,1)
Sys.sleep(t)
}
ua <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.366"
myHttpHeader <- c(
"User-Agent"=ua,
"accept"="text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
"accept-Language"="en-US,en;q=0.9,ja;q=0.8,zh-TW;q=0.7,zh;q=0.6",
"accept-encoding"="gzip, deflate, br",
"cache-control"="max-age=0",
"Connection"="keep-alive",
"Accept-Charset"="UTF8,utf-8;q=0.7,*;q=0.7"
)
curl_handle <- getCurlHandle()
curlSetOpt(cookiejar="cookies.txt",
useragent = ua,
followlocation = TRUE,
curl=curl_handle
)
Sys.getlocale()
Sys.setlocale(category='LC_ALL', locale='C')
data <- list()
i <- 1
for( link in rss$item_link ){
html_doc <- htmlParse(getURL(link, curl = curl_handle), encoding = "UTF-8")
article_item <- xpathSApply(html_doc, "//*[@id='article']/div[2]/div/main/article/div/div[2]/article/div/p", xmlValue)
article_item <- gsub("\\s+", "", article_item)
article_item <- gsub(" $", "", article_item)
data[i] <- article_item
i <- i+1
t <- sample(2:5,1)
Sys.sleep(t)
}
data <- unlist(data)
Sys.setlocale(category='LC_ALL', locale="")
data
ua <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.366"
myHttpHeader <- c(
"User-Agent"=ua,
"accept"="text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
"accept-Language"="zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
"accept-encoding"="gzip, deflate, br",
"Connection"="keep-alive",
"cache-control"="no-cache",
"Accept-Charset"="UTF8,utf-8;q=0.7,*;q=0.7"
)
curl_handle <- getCurlHandle()
curlSetOpt(cookiejar="cookies.txt",
useragent = ua,
followlocation = TRUE,
curl=curl_handle
)
"Accept-Charset"="UTF8,utf-8;q=0.7,*;q=0.7"
"Connection"="keep-alive"
Sys.getlocale()
Sys.setlocale(category='LC_ALL', locale='C')
data <- list()
i <- 1
for( link in rss$item_link ){
html_doc <- htmlParse(getURL(link, curl = curl_handle), encoding = "UTF-8")
article_item <- xpathSApply(html_doc, "//*[@id='article']/div[2]/div/main/article/div/div[2]/article/div/p", xmlValue)
article_item <- gsub("\\s+", "", article_item)
article_item <- gsub(" $", "", article_item)
data[i] <- article_item
i <- i+1
t <- sample(2:5,1)
Sys.sleep(t)
}
Sys.getlocale()
Sys.setlocale(category='LC_ALL', locale='C')
data <- list()
i <- 1
for( link in rss$item_link ){
html_doc <- htmlParse(getURL(link, curl = curl_handle), encoding = "UTF-8")
article_item <- xpathSApply(html_doc, "//div[@class='ndArticle_margin']/p", xmlValue)
article_item <- gsub("\\s+", "", article_item)
article_item <- gsub(" $", "", article_item)
data[i] <- article_item
i <- i+1
t <- sample(2:5,1)
Sys.sleep(t)
}
data
View(rss$item_title)
Sys.setlocale(category='LC_ALL', locale="")
View(rss$item_title)
rss$item_link
Sys.getlocale()
Sys.setlocale(category='LC_ALL', locale='C')
data <- list()
i <- 1
for( link in rss$item_link[0:10] ){
html_doc <- htmlParse(getURL(link, curl = curl_handle), encoding = "UTF-8")
article_item <- xpathSApply(html_doc, "//div[@class='ndArticle_margin']/p", xmlValue)
article_item <- gsub("\\s+", "", article_item)
article_item <- gsub(" $", "", article_item)
data[i] <- article_item
i <- i+1
t <- sample(2:5,1)
Sys.sleep(t)
}
data <- unlist(data)
Sys.setlocale(category='LC_ALL', locale="")
data
Sys.getlocale()
Sys.setlocale(category='LC_ALL', locale='C')
data <- list()
i <- 1
for( link in rss$item_link[0:9] ){
html_doc <- htmlParse(getURL(link, curl = curl_handle), encoding = "UTF-8")
article_item <- xpathSApply(html_doc, "//*[@id="article"]/div[2]/div/main/article/div/div[2]/article/div/p", xmlValue)
article_item <- gsub("\\s+", "", article_item)
article_item <- gsub(" $", "", article_item)
data[i] <- article_item
i <- i+1
t <- sample(2:5,1)
Sys.sleep(t)
}
data <- unlist(data)
Sys.setlocale(category='LC_ALL', locale="")
data
View(rss$item_link)
Sys.getlocale()
Sys.setlocale(category='LC_ALL', locale='C')
data <- list()
i <- 1
for( link in rss$item_link[0:9] ){
html_doc <- htmlParse(getURL(link, curl = curl_handle), encoding = "UTF-8")
article_item <- xpathSApply(html_doc, "//*[@id='article']/div[2]/div/main/article//article/div/p", xmlValue)
article_item <- gsub("\\s+", "", article_item)
article_item <- gsub(" $", "", article_item)
data[i] <- article_item
i <- i+1
t <- sample(2:5,1)
Sys.sleep(t)
}
data <- unlist(data)
Sys.setlocale(category='LC_ALL', locale="")
data\
data
Sys.getlocale()
Sys.setlocale(category='LC_ALL', locale='C')
data <- list()
i <- 1
for( link in rss$item_link[1:10] ){
html_doc <- htmlParse(getURL(link, curl = curl_handle), encoding = "UTF-8")
article_item <- xpathSApply(html_doc, "//*[@id='article']/div[2]/div/main/article//article/div/p", xmlValue)
article_item <- gsub("\\s+", "", article_item)
article_item <- gsub(" $", "", article_item)
data[i] <- article_item
i <- i+1
t <- sample(2:5,1)
Sys.sleep(t)
}
data <- unlist(data)
Sys.setlocale(category='LC_ALL', locale="")
data
data <- list()
i <- 1
for( link in rss$item_link[1:10] ){
html_doc <- htmlParse(getURL(link, curl = curl_handle), encoding = "UTF-8")
article_item <- xpathSApply(html_doc, "//*[@id='article']/div[2]/div/main/article//article/div/p", xmlValue)
# article_item <- gsub("\\s+", "", article_item)
# article_item <- gsub(" $", "", article_item)
data[i] <- article_item
i <- i+1
t <- sample(2:5,1)
Sys.sleep(t)
}
data2 <- list()
i <- 1
for( link in rss$item_link[1:10] ){
html_doc <- htmlParse(getURL(link, curl = curl_handle), encoding = "UTF-8")
article_item <- xpathSApply(html_doc, "//*[@id='article']/div[2]/div/main/article//article/div/p", xmlValue)
article_item <- gsub("\\s+", "", article_item)
article_item <- gsub(" $", "", article_item)
data2[i] <- article_item
i <- i+1
t <- sample(2:5,1)
Sys.sleep(t)
}
View(data)
data
dat2
data2
View(data2)
seg_worker = worker()
seg_worker
library(tidyRSS)
library(XML)
library(RCurl)
library( jiebaR)
library(stringr)
library(plyr)
library(wordcloud)
library(wordcloud2)
# start to parser
library(tidyRSS)
rss_url <- 'https://tw.appledaily.com/rss/newcreate/kind/rnews/type/104'
rss <- tidyRSS::tidyfeed(feed = rss_url)
rss$feed_title # RSS標題
rss$feed_link # RSS超連結
rss$feed_description # 說明
rss$feed_language # 語系
rss$item_title # 文章標題
rss$item_link # 文章超連結
View(rss$item_link)
# Modify by myself
ua <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.366"
myHttpHeader <- c(
"User-Agent"=ua,
"accept"="text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
"accept-Language"="zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
"accept-encoding"="gzip, deflate, br",
"Connection"="keep-alive",
"cache-control"="no-cache",
"Accept-Charset"="UTF8,utf-8;q=0.7,*;q=0.7"
)
curl_handle <- getCurlHandle()
curlSetOpt(cookiejar="cookies.txt",
useragent = ua,
followlocation = TRUE,
curl=curl_handle
)
"Accept-Charset"="UTF8,utf-8;q=0.7,*;q=0.7"
"Connection"="keep-alive"
Sys.getlocale()
Sys.setlocale(category='LC_ALL', locale='C')
data <- list()
i <- 1
for( link in rss$item_link[1:10] ){
html_doc <- htmlParse(getURL(link, curl = curl_handle), encoding = "UTF-8")
article_item <- xpathSApply(html_doc, "//*[@id='article']/div[2]/div/main/article//article/div/p", xmlValue)
article_item <- gsub("\\s+", "", article_item)
article_item <- gsub(" $", "", article_item)
data[i] <- article_item
i <- i+1
t <- sample(2:5,1)
Sys.sleep(t)
}
data <- unlist(data)
Sys.setlocale(category='LC_ALL', locale="")
data
cutter = worker()
seg_words <- cutter <= data
seg_words
cutter
library(plyr)
table(seg_words)
table_words <- count(seg_words)
table_words
install.packages("sqldf")
library(sqldf)
seg_words1 <- as.data.frame(x = seg_words)
seg_words1
table_words
seg_words1
table_words
library(wordcloud)
table(seg_words)
x = data.frame(table(seg_words))
x
wordcloud(x[,1], x[,2], random.order=F)
cutter
cutter = worker(stop_word = "的"
)
seg_words <- cutter <= data
table(seg_words)
x = data.frame(table(seg_words))
x
wordcloud(x[,1], x[,2], random.order=F)
seg_words
table(seg_words)
x = data.frame(table(seg_words))
wordcloud(x[,1], x[,2], random.order=F)
library(plyr)
table(seg_words)
x = data.frame(table(seg_words))
library(wordcloud)
wordcloud(x[,1], x[,2], random.order=F)
table(seg_words)
